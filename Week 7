import os
import numpy as np
from scipy.stats import qmc
from sklearn.kernel_ridge import KernelRidge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

# -----------------------------
# Config (baseline values)
# -----------------------------
FUNCTION_CONFIG = {
    1: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    2: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    3: {"dim": 3, "kappa": 2.4, "n_candidates": 25000},
    4: {"dim": 4, "kappa": 2.6, "n_candidates": 30000},
    5: {"dim": 4, "kappa": 1.4, "n_candidates": 30000},
    6: {"dim": 5, "kappa": 2.6, "n_candidates": 30000},
    7: {"dim": 6, "kappa": 3.0, "n_candidates": 40000},
    8: {"dim": 8, "kappa": 3.2, "n_candidates": 50000},
}

BASE_DIR = "initial_data"

# Portal formatting constraints
CLIP_EPS = 1e-6  # keep strictly < 1.0 so each value starts with "0."

# Round-7 tuning grids (small but meaningful)
ALPHA_GRID = np.array([1e-4, 1e-3, 1e-2, 1e-1], dtype=float)
GAMMA_GRID = np.array([0.25, 0.5, 1.0, 2.0, 4.0], dtype=float)

# Base strategy knobs (will be adapted using noise estimates)
BASE_ENSEMBLE_B = 12
BASE_LOCAL_FRAC = 0.35
BASE_LOCAL_NOISE = 0.07

# CV settings
CV_SPLITS = 5
CV_SEED = 123


# -----------------------------
# Portal formatting
# -----------------------------
def format_query(x):
    x = np.clip(x, 0.0, 1.0 - CLIP_EPS)
    return "-".join(f"{float(v):.6f}" for v in x)


# -----------------------------
# Candidate generation
# -----------------------------
def sobol_candidates(n, dim, seed):
    m = int(np.ceil(np.log2(n)))
    sampler = qmc.Sobol(d=dim, scramble=True, seed=seed)
    X = sampler.random(n=2**m)
    return X[:n]

def local_candidates(x_center, n, seed, noise):
    rng = np.random.default_rng(seed)
    X = x_center[None, :] + rng.normal(0.0, noise, size=(n, x_center.size))
    return np.clip(X, 0.0, 1.0 - CLIP_EPS)


# -----------------------------
# Helper: estimate "noise level" from simple CV residuals
# -----------------------------
def estimate_noise_level(Xs, y, alpha, gamma, seed):
    """
    Returns a scalar noise proxy in [0, +inf). Larger => noisier / harder function.
    """
    n = Xs.shape[0]
    if n < 6:
        return 0.0

    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)
    errs = []
    for tr, va in kf.split(Xs):
        model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
        model.fit(Xs[tr], y[tr])
        pred = model.predict(Xs[va])
        errs.append(np.mean((pred - y[va]) ** 2))
    return float(np.mean(errs))


# -----------------------------
# Hyperparameter tuning (alpha, gamma) via CV
# -----------------------------
def tune_krr_hyperparams(Xs, y, seed):
    """
    Small grid search to choose (alpha, gamma) that minimises CV MSE.
    """
    best = {"alpha": None, "gamma": None, "cv_mse": np.inf}

    n = Xs.shape[0]
    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)

    for alpha in ALPHA_GRID:
        for gamma in GAMMA_GRID:
            mses = []
            for tr, va in kf.split(Xs):
                model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
                model.fit(Xs[tr], y[tr])
                pred = model.predict(Xs[va])
                mses.append(np.mean((pred - y[va]) ** 2))
            cv_mse = float(np.mean(mses))
            if cv_mse < best["cv_mse"]:
                best = {"alpha": float(alpha), "gamma": float(gamma), "cv_mse": cv_mse}

    return best


# -----------------------------
# Kernel Ridge ensemble UCB (now with tuned alpha/gamma + adaptive BO knobs)
# -----------------------------
def kernel_ucb_suggest(X, y, dim, kappa_base, n_candidates, seed):
    rng = np.random.default_rng(seed)

    # Scale inputs
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X)

    # 1) Tune surrogate hyperparameters on current data
    tuned = tune_krr_hyperparams(Xs, y, seed=seed)

    alpha_star = tuned["alpha"]
    gamma_star = tuned["gamma"]

    # 2) Noise proxy -> adapt exploration/exploitation hyperparameters
    noise_proxy = estimate_noise_level(Xs, y, alpha_star, gamma_star, seed=seed)

    # Heuristic adaptation rules (simple, explainable)
    # - noisier => explore more (higher kappa), reduce local exploitation, increase ensemble
    # - smoother => exploit more (lower kappa), increase local candidates
    # clamp everything to sane ranges
    noise_norm = noise_proxy / (noise_proxy + 1.0)  # in (0,1)

    ENSEMBLE_B = int(np.clip(BASE_ENSEMBLE_B + 10 * noise_norm, 10, 24))
    LOCAL_FRAC = float(np.clip(BASE_LOCAL_FRAC - 0.20 * noise_norm, 0.15, 0.45))
    LOCAL_NOISE = float(np.clip(BASE_LOCAL_NOISE + 0.06 * noise_norm, 0.05, 0.14))
    kappa = float(np.clip(kappa_base + 0.8 * noise_norm, 0.8, 4.0))

    # Best observed point -> local anchor
    best_idx = int(np.argmax(y))
    x_best = X[best_idx]

    # Candidate mix
    n_local = int(n_candidates * LOCAL_FRAC)
    n_global = n_candidates - n_local

    X_global = sobol_candidates(n_global, dim, seed)
    X_local = local_candidates(x_best, n_local, seed + 999, noise=LOCAL_NOISE)
    X_cand = np.vstack([X_global, X_local])
    Xc = scaler.transform(X_cand)

    # 3) Bootstrap ensemble using tuned hyperparameters
    preds = np.zeros((ENSEMBLE_B, Xc.shape[0]), dtype=float)

    n = Xs.shape[0]
    for b in range(ENSEMBLE_B):
        idx = rng.integers(0, n, size=n)
        Xb = Xs[idx]
        yb = y[idx]

        model = KernelRidge(kernel="rbf", alpha=alpha_star, gamma=gamma_star)
        model.fit(Xb, yb)
        preds[b] = model.predict(Xc)

    mu = preds.mean(axis=0)
    sigma = preds.std(axis=0)

    # Extra diversity / "donâ€™t sample exactly on top of old points"
    dists = np.min(np.linalg.norm(Xc[:, None, :] - Xs[None, :, :], axis=2), axis=1)
    dists = dists / (dists.max() + 1e-12)
    sigma = 0.85 * sigma + 0.15 * dists

    # 4) UCB acquisition
    acq = mu + kappa * sigma
    x_next = X_cand[int(np.argmax(acq))]
    x_next = np.clip(x_next, 0.0, 1.0 - CLIP_EPS)

    debug = {
        "alpha": alpha_star,
        "gamma": gamma_star,
        "cv_mse": tuned["cv_mse"],
        "noise_proxy": noise_proxy,
        "noise_norm": noise_norm,
        "ENSEMBLE_B": ENSEMBLE_B,
        "LOCAL_FRAC": LOCAL_FRAC,
        "LOCAL_NOISE": LOCAL_NOISE,
        "kappa_used": kappa,
    }
    return x_next, debug


# -----------------------------
# One function runner
# -----------------------------
def suggest_next_point_for_function(func_id, seed):
    cfg = FUNCTION_CONFIG[func_id]
    dim = cfg["dim"]

    folder = os.path.join(BASE_DIR, f"function_{func_id}")
    X = np.load(os.path.join(folder, "initial_inputs.npy"))
    y = np.load(os.path.join(folder, "initial_outputs.npy")).ravel()

    if X.shape[1] != dim:
        raise ValueError(f"Dim mismatch for function {func_id}: expected {dim}, got {X.shape[1]}")

    x_next, debug = kernel_ucb_suggest(
        X, y,
        dim=dim,
        kappa_base=cfg["kappa"],
        n_candidates=cfg["n_candidates"],
        seed=seed
    )

    return x_next, format_query(x_next), debug


# -----------------------------
# Run all functions
# -----------------------------
def suggest_for_all_functions():
    results = {}
    debug_all = {}

    for func_id in range(1, 9):
        x_next, submission, debug = suggest_next_point_for_function(func_id, seed=func_id)

        results[func_id] = submission
        debug_all[func_id] = debug

        print(f"Function {func_id}: {submission}")
        print(f"  tuned alpha={debug['alpha']}, gamma={debug['gamma']}, cv_mse={debug['cv_mse']:.6g}")
        print(f"  noise_proxy={debug['noise_proxy']:.6g} -> kappa_used={debug['kappa_used']:.3f}")
        print(f"  ENSEMBLE_B={debug['ENSEMBLE_B']}, LOCAL_FRAC={debug['LOCAL_FRAC']:.2f}, LOCAL_NOISE={debug['LOCAL_NOISE']:.3f}")
        print()

    return results, debug_all


if __name__ == "__main__":
    suggest_for_all_functions()
