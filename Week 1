import numpy as np

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel

# NEW: better candidate sampling
from scipy.stats import qmc

# OPTIONAL: if your X isn't guaranteed in [0,1]
from sklearn.preprocessing import MinMaxScaler


# -----------------------------
# Config: dimensions & kappa per function
# -----------------------------
FUNCTION_CONFIG = {
    1: {"dim": 2, "kappa": 2.0, "n_candidates": 20000},
    2: {"dim": 2, "kappa": 2.0, "n_candidates": 20000},
    3: {"dim": 3, "kappa": 2.2, "n_candidates": 25000},
    4: {"dim": 4, "kappa": 2.5, "n_candidates": 30000},
    5: {"dim": 4, "kappa": 1.2, "n_candidates": 30000},
    6: {"dim": 5, "kappa": 2.7, "n_candidates": 30000},
    7: {"dim": 6, "kappa": 3.0, "n_candidates": 40000},
    8: {"dim": 8, "kappa": 3.2, "n_candidates": 50000},
}

BASE_DIR = "initial_data"  # folder containing function_1, ... function_8

# -----------------------------
# Global toggles
# -----------------------------
USE_SOBOL = True              # better space-filling candidate set
ENFORCE_MIN_DIST = True       # avoid proposing points too close to existing data
MIN_DIST_FRAC = 0.02          # min distance as fraction of sqrt(dim) (reasonable default)
SCALE_INPUTS_IF_NEEDED = False  # set True if your X isn't already in [0,1]


# -----------------------------
# Format for portal submission
# -----------------------------
def format_query(x):
    return "-".join(f"{float(v):.6f}" for v in x)


# -----------------------------
# UCB acquisition function
# -----------------------------
def ucb_acquisition(X_cand, gp, kappa=2.5):
    mu, std = gp.predict(X_cand, return_std=True)
    return mu + kappa * std


# -----------------------------
# Helper: generate candidates in [0,1]^dim (Sobol or uniform)
# -----------------------------
def generate_candidates(n_candidates: int, dim: int, seed: int) -> np.ndarray:
    if USE_SOBOL:
        # Sobol works best with power-of-two sizes; we oversample then trim.
        m = int(np.ceil(np.log2(n_candidates)))
        n = 2 ** m
        sampler = qmc.Sobol(d=dim, scramble=True, seed=seed)
        X = sampler.random(n=n)  # already in [0,1]
        return X[:n_candidates]
    else:
        rng = np.random.default_rng(seed)
        return rng.uniform(0.0, 1.0, size=(n_candidates, dim))


# -----------------------------
# Helper: enforce minimum distance from existing samples
# -----------------------------
def filter_by_min_distance(X_cand: np.ndarray, X_obs: np.ndarray, min_dist: float) -> np.ndarray:
    # Compute min Euclidean distance from each candidate to observed points
    # Efficient enough for your sizes; if needed we can swap to sklearn NearestNeighbors.
    dists = np.sqrt(((X_cand[:, None, :] - X_obs[None, :, :]) ** 2).sum(axis=2))
    min_d = dists.min(axis=1)
    return X_cand[min_d >= min_dist]


# -----------------------------
# Core BO helper for ONE function_k
# -----------------------------
def suggest_next_point_for_function(func_id, random_state=42):
    if func_id not in FUNCTION_CONFIG:
        raise ValueError(f"Unknown function id {func_id}. Must be 1–8.")

    cfg = FUNCTION_CONFIG[func_id]
    dim = cfg["dim"]
    kappa = cfg["kappa"]
    n_candidates = cfg["n_candidates"]

    # 1. Load data for this function
    folder = f"{BASE_DIR}/function_{func_id}"
    X_path = f"{folder}/initial_inputs.npy"
    y_path = f"{folder}/initial_outputs.npy"

    X = np.load(X_path)
    y = np.load(y_path).ravel()

    if X.ndim != 2:
        raise ValueError(f"Function {func_id}: X should be 2D, got shape {X.shape}")
    if X.shape[1] != dim:
        raise ValueError(f"Function {func_id}: expected dim={dim}, but X shape is {X.shape}")

    # Optional: scale to [0,1] if your data isn't already there
    scaler = None
    X_fit = X
    if SCALE_INPUTS_IF_NEEDED:
        scaler = MinMaxScaler()
        X_fit = scaler.fit_transform(X)

    # 2. Define GP kernel (ARD length-scales: one per dimension)
    length_scale0 = np.full(dim, 0.2)
    kernel = (
        ConstantKernel(1.0, (0.01, 10.0))
        * Matern(
            length_scale=length_scale0,
            length_scale_bounds=(1e-4, 10.0),
            nu=2.5
        )
        + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1.0))
    )

    gp = GaussianProcessRegressor(
        kernel=kernel,
        normalize_y=True,
        n_restarts_optimizer=5,
        random_state=random_state,
        alpha=1e-10,  # numerical jitter for stability (especially if y is nearly noise-free)
    )
    gp.fit(X_fit, y)

    # 3. Sample candidate points in [0,1]^dim using Sobol (better coverage in high-D)
    X_cand = generate_candidates(n_candidates, dim, seed=random_state)

    # If we scaled inputs, candidates are already in [0,1] which matches X_fit space
    # If we didn't scale, we assume X was already in [0,1] (as per your original approach)

    # 3b. Enforce min distance from existing samples (avoid near-duplicates)
    if ENFORCE_MIN_DIST:
        # Scale-aware min_dist: expressed as fraction of diagonal of unit hypercube
        diag = np.sqrt(dim)
        min_dist = MIN_DIST_FRAC * diag
        X_cand_f = filter_by_min_distance(X_cand, X_fit, min_dist=min_dist)

        # If filtering is too aggressive, fall back to unfiltered candidates
        if X_cand_f.shape[0] >= max(2000, n_candidates // 10):
            X_cand = X_cand_f

    # 4. Evaluate acquisition
    acq_values = ucb_acquisition(X_cand, gp, kappa=kappa)
    best_idx = int(np.argmax(acq_values))
    x_next_fit = np.clip(X_cand[best_idx], 0.0, 1.0)

    # If we scaled, invert transform back to original space for submission (if needed)
    if scaler is not None:
        x_next = scaler.inverse_transform(x_next_fit.reshape(1, -1)).ravel()
    else:
        x_next = x_next_fit

    submission = format_query(x_next)

    return x_next, submission, gp


# -----------------------------
# Run for all functions 1–8
# -----------------------------
def suggest_for_all_functions():
    results = {}
    for func_id in range(1, 9):
        x_next, submission, _ = suggest_next_point_for_function(
            func_id,
            random_state=func_id  # keep deterministic but different per function
        )
        results[func_id] = {"x_next": x_next, "submission": submission}
        print(f"Function {func_id}:")
        print("  x_next      =", x_next)
        print("  submission  =", submission)
        print()
    return results


results = suggest_for_all_functions()
