# =============================
# BBO Capstone: Cluster-aware, transparent query suggestion (Round 11-ready)
# Adds: clustering lens (Agglomerative/Ward + silhouette), cluster-anchored local sampling,
# cluster-aware selection nudges, and audit logs + reflection snippets.
# =============================

import os
import json
import math
import datetime as dt
from pathlib import Path

import numpy as np
from scipy.stats import qmc

from sklearn.kernel_ridge import KernelRidge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score


# -----------------------------
# Config (baseline values)
# -----------------------------
FUNCTION_CONFIG = {
    1: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    2: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    3: {"dim": 3, "kappa": 2.4, "n_candidates": 25000},
    4: {"dim": 4, "kappa": 2.6, "n_candidates": 30000},
    5: {"dim": 4, "kappa": 1.4, "n_candidates": 30000},
    6: {"dim": 5, "kappa": 2.6, "n_candidates": 30000},
    7: {"dim": 6, "kappa": 3.0, "n_candidates": 40000},
    8: {"dim": 8, "kappa": 3.2, "n_candidates": 50000},
}

BASE_DIR = "initial_data"
CLIP_EPS = 1e-6  # keep strictly < 1.0 so each value starts with "0."

# Hyperparameter tuning grids (small but meaningful)
ALPHA_GRID = np.array([1e-4, 1e-3, 1e-2, 1e-1], dtype=float)
GAMMA_GRID = np.array([0.25, 0.5, 1.0, 2.0, 4.0], dtype=float)

# CV settings
CV_SPLITS = 5

# Logging / GitHub artefacts
ARTIFACTS_DIR = Path("artifacts_stage2_cluster")
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# -----------------------------
# "Transformer-inspired" knobs
# -----------------------------
DECODER = {
    # base decoding behaviour
    "temperature_base": 0.65,
    "top_p_base": 0.15,
    "top_k_base": 250,

    # attention-like local anchoring
    "attn_top_m": 3,
    "attn_softmax_temp": 0.25,

    # best-of-N sampling
    "best_of": 8,

    # risk control (higher => more conservative)
    "risk_lambda_base": 0.35,
    "dist_lambda": 0.12,
    "edge_lambda": 0.08,

    # clustering behaviour
    "cluster_k_min": 2,
    "cluster_k_max": 6,
    "cluster_bonus": 0.03,  # small nudge to cover under-sampled clusters
}

# Base BO knobs
BASE_ENSEMBLE_B = 12
BASE_LOCAL_FRAC = 0.35
BASE_LOCAL_NOISE = 0.07


# -----------------------------
# File helpers
# -----------------------------
def now_utc_iso():
    return dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

def save_json(obj, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, sort_keys=True)

def save_text(text, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)


# -----------------------------
# Utilities
# -----------------------------
def format_query(x):
    x = np.clip(x, 0.0, 1.0 - CLIP_EPS)
    return "-".join(f"{float(v):.6f}" for v in x)

def sobol_candidates(n, dim, seed):
    m = int(np.ceil(np.log2(max(n, 2))))
    sampler = qmc.Sobol(d=dim, scramble=True, seed=seed)
    X = sampler.random(n=2**m)
    return X[:n]

def local_candidates(x_center, n, seed, noise):
    rng = np.random.default_rng(seed)
    X = x_center[None, :] + rng.normal(0.0, noise, size=(n, x_center.size))
    return np.clip(X, 0.0, 1.0 - CLIP_EPS)

def softmax(z, temp=1.0):
    z = (z - np.max(z)) / max(temp, 1e-12)
    e = np.exp(z)
    return e / (np.sum(e) + 1e-12)

def boundary_penalty(X):
    """
    Penalise points very close to 0 or 1.
    """
    d = np.minimum(X, 1.0 - X)
    return (1.0 / (d + 1e-6)).mean(axis=1)


# -----------------------------
# Noise proxy + tuning
# -----------------------------
def estimate_noise_level(Xs, y, alpha, gamma, seed):
    n = Xs.shape[0]
    if n < 6:
        return 0.0
    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)
    errs = []
    for tr, va in kf.split(Xs):
        model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
        model.fit(Xs[tr], y[tr])
        pred = model.predict(Xs[va])
        errs.append(np.mean((pred - y[va]) ** 2))
    return float(np.mean(errs))

def tune_krr_hyperparams(Xs, y, seed):
    best = {"alpha": None, "gamma": None, "cv_mse": np.inf}
    n = Xs.shape[0]
    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)

    for alpha in ALPHA_GRID:
        for gamma in GAMMA_GRID:
            mses = []
            for tr, va in kf.split(Xs):
                model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
                model.fit(Xs[tr], y[tr])
                pred = model.predict(Xs[va])
                mses.append(np.mean((pred - y[va]) ** 2))
            cv_mse = float(np.mean(mses))
            if cv_mse < best["cv_mse"]:
                best = {"alpha": float(alpha), "gamma": float(gamma), "cv_mse": cv_mse}
    return best


# -----------------------------
# Clustering helpers (Agglomerative/Ward + silhouette)
# -----------------------------
def pick_k_via_silhouette(Xs, k_min=2, k_max=6, seed=0):
    n = Xs.shape[0]
    k_max = min(k_max, n - 1)
    if n < 4 or k_max < k_min:
        return 1, None, None

    best_k, best_s, best_labels = None, -np.inf, None
    for k in range(k_min, k_max + 1):
        labels = AgglomerativeClustering(n_clusters=k, linkage="ward").fit_predict(Xs)
        if len(np.unique(labels)) < 2:
            continue
        s = silhouette_score(Xs, labels)
        if s > best_s:
            best_s, best_k, best_labels = s, k, labels

    if best_k is None:
        return 1, None, None
    return int(best_k), best_labels, float(best_s)

def compute_cluster_stats(X, y, scaler, labels):
    """
    Returns centroids (original space) and simple per-cluster summaries.
    """
    Xs = scaler.transform(X)
    clusters = np.unique(labels)
    k = len(clusters)

    centroids_s = np.vstack([Xs[labels == c].mean(axis=0) for c in clusters])
    centroids = scaler.inverse_transform(centroids_s)

    stats = []
    for c in clusters:
        idx = np.where(labels == c)[0]
        yc = y[idx]
        stats.append({
            "cluster_id": int(c),
            "n_points": int(len(idx)),
            "best_y": float(np.max(yc)),
            "mean_y": float(np.mean(yc)),
            "std_y": float(np.std(yc)),
        })
    return centroids, stats


# -----------------------------
# Decoding over candidates (temperature / top-p / top-k)
# -----------------------------
def decode_choice(scores, rng, temperature, top_p, top_k):
    probs = softmax(scores, temp=max(temperature, 1e-6))

    order = np.argsort(-probs)
    probs_sorted = probs[order]

    k = min(int(top_k), len(order))
    order = order[:k]
    probs_sorted = probs_sorted[:k]

    cdf = np.cumsum(probs_sorted)
    cutoff = int(np.searchsorted(cdf, top_p))
    cutoff = int(np.clip(cutoff, 0, len(probs_sorted) - 1))
    keep = cutoff + 1

    order = order[:keep]
    probs_keep = probs_sorted[:keep]
    probs_keep = probs_keep / (np.sum(probs_keep) + 1e-12)

    return int(rng.choice(order, p=probs_keep))


# -----------------------------
# Kernel Ridge ensemble UCB + transformer-style decoding + cluster-aware local search
# -----------------------------
def kernel_ucb_suggest(X, y, dim, kappa_base, n_candidates_base, seed):
    rng = np.random.default_rng(seed)

    n_obs = X.shape[0]
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X)

    # 1) tune surrogate hyperparams
    tuned = tune_krr_hyperparams(Xs, y, seed=seed)
    alpha_star, gamma_star = tuned["alpha"], tuned["gamma"]

    # 2) noise proxy
    noise_proxy = estimate_noise_level(Xs, y, alpha_star, gamma_star, seed=seed)
    noise_norm = noise_proxy / (noise_proxy + 1.0)

    # 3) scale budgets with data + noise
    scale_with_data = np.clip((n_obs - 10) / 12.0, 0.0, 1.0)
    n_candidates = int(np.clip(
        n_candidates_base * (1.0 + 0.25 * scale_with_data + 0.25 * noise_norm),
        8000,
        70000
    ))

    ENSEMBLE_B = int(np.clip(BASE_ENSEMBLE_B + 6 * scale_with_data + 10 * noise_norm, 10, 28))
    LOCAL_FRAC = float(np.clip(BASE_LOCAL_FRAC - 0.15 * noise_norm + 0.05 * (1 - scale_with_data), 0.15, 0.45))
    LOCAL_NOISE = float(np.clip(BASE_LOCAL_NOISE + 0.06 * noise_norm, 0.05, 0.16))

    # 4) exploration strength (UCB)
    kappa = float(np.clip(kappa_base + 0.9 * noise_norm - 0.2 * scale_with_data, 0.8, 4.0))

    # 5) decoding knobs
    temperature = float(np.clip(
        DECODER["temperature_base"] + 0.45 * noise_norm - 0.25 * scale_with_data,
        0.35, 1.15
    ))
    top_p = float(np.clip(
        DECODER["top_p_base"] + 0.12 * noise_norm - 0.05 * scale_with_data,
        0.08, 0.35
    ))
    top_k = int(np.clip(
        DECODER["top_k_base"] + 350 * noise_norm + 200 * (1 - scale_with_data),
        120, 900
    ))

    # 6) risk penalty strengthens with more data
    risk_lambda = float(np.clip(
        DECODER["risk_lambda_base"] + 0.20 * scale_with_data,
        0.20, 0.70
    ))

    # 7) CLUSTERING LENS on observed points
    cluster_k, cluster_labels, sil = pick_k_via_silhouette(
        Xs,
        k_min=DECODER["cluster_k_min"],
        k_max=DECODER["cluster_k_max"],
        seed=seed
    )
    cluster_centroids = None
    cluster_stats = None
    if cluster_k > 1 and cluster_labels is not None:
        cluster_centroids, cluster_stats = compute_cluster_stats(X, y, scaler, cluster_labels)

    # 8) Candidate generation: global Sobol + local around cluster centroids
    n_local = int(n_candidates * LOCAL_FRAC)
    n_global = n_candidates - n_local

    X_global = sobol_candidates(n_global, dim, seed)
    locals_list = []

    if n_local > 0:
        if cluster_centroids is not None:
            # allocate local budget per cluster: exploit (best_y) + explore (inverse count)
            perf = np.array([cs["best_y"] for cs in cluster_stats], dtype=float)
            perf = (perf - perf.min()) / (perf.max() - perf.min() + 1e-12)

            counts = np.array([cs["n_points"] for cs in cluster_stats], dtype=float)
            inv_counts = 1.0 / (counts + 1e-6)
            inv_counts = inv_counts / (inv_counts.sum() + 1e-12)

            w_exploit = np.clip(0.55 + 0.25 * scale_with_data - 0.20 * noise_norm, 0.25, 0.85)
            w_explore = 1.0 - w_exploit

            weights = w_exploit * perf + w_explore * inv_counts
            weights = weights / (weights.sum() + 1e-12)

            alloc = np.maximum(1, (weights * n_local).astype(int))
            while alloc.sum() > n_local:
                alloc[np.argmax(alloc)] -= 1
            while alloc.sum() < n_local:
                alloc[np.argmax(weights)] += 1

            for j in range(cluster_k):
                locals_list.append(
                    local_candidates(cluster_centroids[j], int(alloc[j]), seed + 3000 + j, noise=LOCAL_NOISE)
                )
        else:
            # fallback: local around top-m observed points
            m = min(DECODER["attn_top_m"], len(y))
            top_idx = np.argsort(-y)[:m]
            weights = softmax(y[top_idx], temp=DECODER["attn_softmax_temp"])
            alloc = np.maximum(1, (weights * n_local).astype(int))
            while alloc.sum() > n_local:
                alloc[np.argmax(alloc)] -= 1
            while alloc.sum() < n_local:
                alloc[np.argmax(weights)] += 1
            for j, idx in enumerate(top_idx):
                locals_list.append(local_candidates(X[idx], int(alloc[j]), seed + 1000 + j, noise=LOCAL_NOISE))

    X_local = np.vstack(locals_list) if locals_list else np.empty((0, dim))
    X_cand = np.vstack([X_global, X_local])
    X_cand = np.clip(X_cand, 0.0, 1.0 - CLIP_EPS)
    Xc = scaler.transform(X_cand)

    # 9) ensemble predictions => mu, sigma
    preds = np.zeros((ENSEMBLE_B, Xc.shape[0]), dtype=float)
    n = Xs.shape[0]

    for b in range(ENSEMBLE_B):
        idx = rng.integers(0, n, size=n)
        model = KernelRidge(kernel="rbf", alpha=alpha_star, gamma=gamma_star)
        model.fit(Xs[idx], y[idx])
        preds[b] = model.predict(Xc)

    mu = preds.mean(axis=0)
    sigma = preds.std(axis=0)

    # 10) distance-to-training + boundary penalty
    dists = np.min(np.linalg.norm(Xc[:, None, :] - Xs[None, :, :], axis=2), axis=1)
    dists_norm = dists / (dists.max() + 1e-12)

    edge_pen = boundary_penalty(X_cand)
    edge_pen = edge_pen / (edge_pen.max() + 1e-12)

    # 11) baseline acquisition (UCB)
    ucb = mu + kappa * (0.85 * sigma + 0.15 * dists_norm)

    # 12) risk-adjusted score
    risk_adjusted = (
        ucb
        - risk_lambda * sigma
        - DECODER["dist_lambda"] * dists_norm
        - DECODER["edge_lambda"] * edge_pen
    )

    # 13) cluster-aware nudge: prefer candidates in under-sampled clusters (small bonus)
    target_cluster_id = None
    target_reason = None
    dist_to_target_centroid = None

    if cluster_centroids is not None:
        cent_s = scaler.transform(cluster_centroids)
        dist_mat = np.linalg.norm(Xc[:, None, :] - cent_s[None, :, :], axis=2)
        nearest = dist_mat.argmin(axis=1)
        nearest_dist = dist_mat.min(axis=1)

        counts = np.array([cs["n_points"] for cs in cluster_stats], dtype=float)
        inv = 1.0 / (counts + 1e-6)
        cluster_bonus = inv[nearest]
        cluster_bonus = cluster_bonus / (cluster_bonus.max() + 1e-12)

        risk_adjusted = risk_adjusted + DECODER["cluster_bonus"] * cluster_bonus
    else:
        nearest = None
        nearest_dist = None

    # 14) decoding-style shortlist + best-of-N, then pick best risk_adjusted among shortlist
    best_of = int(np.clip(DECODER["best_of"], 3, 20))
    sampled = [decode_choice(ucb, rng, temperature, top_p, top_k) for _ in range(best_of)]
    sampled = np.unique(sampled)

    chosen_idx = int(sampled[np.argmax(risk_adjusted[sampled])])
    x_next = np.clip(X_cand[chosen_idx], 0.0, 1.0 - CLIP_EPS)

    # determine cluster targeting info for explainability
    if cluster_centroids is not None and nearest is not None:
        target_cluster_id = int(nearest[chosen_idx])
        dist_to_target_centroid = float(nearest_dist[chosen_idx])

        chosen_count = cluster_stats[target_cluster_id]["n_points"]
        min_count = min(cs["n_points"] for cs in cluster_stats)
        if chosen_count == min_count:
            target_reason = "exploration: under-sampled cluster"
        else:
            target_reason = "exploitation: high-performing cluster region"

    debug = {
        "n_obs": int(n_obs),
        "n_candidates_used": int(n_candidates),
        "alpha": float(alpha_star),
        "gamma": float(gamma_star),
        "cv_mse": float(tuned["cv_mse"]),
        "noise_proxy": float(noise_proxy),
        "noise_norm": float(noise_norm),
        "ENSEMBLE_B": int(ENSEMBLE_B),
        "LOCAL_FRAC": float(LOCAL_FRAC),
        "LOCAL_NOISE": float(LOCAL_NOISE),
        "kappa_used": float(kappa),
        "temperature": float(temperature),
        "top_p": float(top_p),
        "top_k": int(top_k),
        "best_of": int(best_of),
        "risk_lambda": float(risk_lambda),

        # clustering lens
        "cluster_k": int(cluster_k),
        "cluster_silhouette": None if sil is None else float(sil),
        "cluster_summary": cluster_stats,  # list of dicts or None
        "target_cluster_id": target_cluster_id,
        "target_reason": target_reason,
        "dist_to_target_centroid_scaled": dist_to_target_centroid,
    }
    return x_next, debug


# -----------------------------
# One function runner (with logging)
# -----------------------------
def suggest_next_point_for_function(func_id, seed, expected_n_obs=None, round_label="Round 11"):
    cfg = FUNCTION_CONFIG[func_id]
    dim = cfg["dim"]

    folder = os.path.join(BASE_DIR, f"function_{func_id}")
    X = np.load(os.path.join(folder, "initial_inputs.npy"))
    y = np.load(os.path.join(folder, "initial_outputs.npy")).ravel()

    if X.shape[1] != dim:
        raise ValueError(f"Dim mismatch for function {func_id}: expected {dim}, got {X.shape[1]}")

    if expected_n_obs is not None and X.shape[0] != expected_n_obs:
        print(f"[WARN] Function {func_id}: expected {expected_n_obs} observations, found {X.shape[0]}")

    x_next, debug = kernel_ucb_suggest(
        X, y,
        dim=dim,
        kappa_base=cfg["kappa"],
        n_candidates_base=cfg["n_candidates"],
        seed=seed
    )

    submission = format_query(x_next)

    record = {
        "timestamp_utc": now_utc_iso(),
        "stage": "Stage 2",
        "round": round_label,
        "round_context": (
            "Preparing next query with a clustering lens: identify natural groupings of prior queries "
            "and target a cluster based on centroid trends / distances / boundary tightening."
        ),
        "function_id": int(func_id),
        "dim": int(dim),
        "n_obs": int(X.shape[0]),
        "candidate_budget_base": int(cfg["n_candidates"]),
        "kappa_base": float(cfg["kappa"]),
        "suggested_x": [float(v) for v in x_next.tolist()],
        "submission_string": submission,
        "debug": debug,
        "reproducibility": {
            "seed": int(seed),
            "clip_eps": float(CLIP_EPS),
            "alpha_grid": ALPHA_GRID.tolist(),
            "gamma_grid": GAMMA_GRID.tolist(),
            "cv_splits": int(CV_SPLITS),
        },
    }

    # Save record per function
    save_json(record, ARTIFACTS_DIR / f"function_{func_id}" / "run_record.json")

    return x_next, submission, debug, record


# -----------------------------
# Run all functions (with consolidated logging)
# -----------------------------
def suggest_for_all_functions(expected_n_obs=None, round_label="Round 11"):
    results = {}
    debug_all = {}
    records_all = {}

    for func_id in range(1, 9):
        x_next, submission, debug, record = suggest_next_point_for_function(
            func_id, seed=func_id, expected_n_obs=expected_n_obs, round_label=round_label
        )

        results[func_id] = submission
        debug_all[func_id] = debug
        records_all[func_id] = record

        print(f"Function {func_id}: {submission}")
        print(f"  n_obs={debug['n_obs']}  n_candidates={debug['n_candidates_used']}  ensemble={debug['ENSEMBLE_B']}")
        print(f"  tuned alpha={debug['alpha']} gamma={debug['gamma']} cv_mse={debug['cv_mse']:.6g}")
        print(f"  noise_proxy={debug['noise_proxy']:.6g} -> kappa={debug['kappa_used']:.3f}")
        print(f"  local_frac={debug['LOCAL_FRAC']:.2f} local_noise={debug['LOCAL_NOISE']:.3f}")
        print(f"  decoder: temp={debug['temperature']:.3f} top_p={debug['top_p']:.3f} top_k={debug['top_k']} best_of={debug['best_of']}")
        print(f"  risk_lambda={debug['risk_lambda']:.3f}")
        print(f"  clustering: k={debug['cluster_k']} sil={debug['cluster_silhouette']} target_cluster={debug['target_cluster_id']} reason={debug['target_reason']}")
        print()

    save_json(
        {"timestamp_utc": now_utc_iso(), "results": results, "records": records_all},
        ARTIFACTS_DIR / "stage2_round11_next_queries.json"
    )

    # Optional: also save a plain text file for quick copy/paste into portal
    lines = [f"Function {fid}: {q}" for fid, q in results.items()]
    save_text("\n".join(lines) + "\n", ARTIFACTS_DIR / "submission_strings.txt")

    return results, debug_all, records_all


def make_reflection_snippet(record):
    d = record["debug"]

    cluster_line = ""
    if d.get("cluster_k", 1) > 1 and d.get("target_cluster_id") is not None:
        cluster_line = (
            f" We clustered the observed points into k={d['cluster_k']} groups (silhouette={d['cluster_silhouette']:.3f})."
            f" This query targets cluster {d['target_cluster_id']} based on {d['target_reason']}."
            f" Similarity cue: dist-to-centroid(scaled)={d['dist_to_target_centroid_scaled']:.3f}."
        )
    else:
        cluster_line = " Clustering was not stable (insufficient structure), so we fell back to local anchors around best observed points."

    return (
        f"Function {record['function_id']} (d={record['dim']}): We used an RBF Kernel Ridge surrogate with CV-tuned "
        f"alpha={d['alpha']}, gamma={d['gamma']} (CV MSE={d['cv_mse']:.4g}). With n={d['n_obs']} observations, "
        f"we estimated noise_proxy={d['noise_proxy']:.4g}, setting exploration kappa={d['kappa_used']:.2f}. "
        f"Candidates were generated via Sobol global sampling + local sampling (local_frac={d['LOCAL_FRAC']:.2f}). "
        f"Selection used UCB plus risk controls (risk_lambda={d['risk_lambda']:.2f}) and boundary penalties, "
        f"then a shortlist decoding step (temp={d['temperature']:.2f}, top_p={d['top_p']:.2f}, top_k={d['top_k']}, best_of={d['best_of']})."
        f"{cluster_line} Proposed submission: {record['submission_string']}."
    )


if __name__ == "__main__":
    # If you *know* the expected obs count for Round 11, set it here (e.g., 20 or 21).
    # Otherwise leave as None to just warn-free.
    results, debug_all, records_all = suggest_for_all_functions(expected_n_obs=None, round_label="Round 11")

    # Print reflection snippets (useful for discussion board justification)
    for fid in range(1, 9):
        print(make_reflection_snippet(records_all[fid]))
        print()
