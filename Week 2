import numpy as np

# --- GP bits (good for low/mid dims) ---
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel
from scipy.stats import qmc

# --- NN bits (good for high dims) ---
import tensorflow as tf
from tensorflow.keras import layers, models

# -----------------------------
# Config
# -----------------------------
FUNCTION_CONFIG = {
    1: {"dim": 2, "kappa": 2.0, "n_candidates": 20000},
    2: {"dim": 2, "kappa": 2.0, "n_candidates": 20000},
    3: {"dim": 3, "kappa": 2.2, "n_candidates": 25000},
    4: {"dim": 4, "kappa": 2.5, "n_candidates": 30000},
    5: {"dim": 4, "kappa": 1.2, "n_candidates": 30000},
    6: {"dim": 5, "kappa": 2.7, "n_candidates": 30000},
    7: {"dim": 6, "kappa": 3.0, "n_candidates": 40000},
    8: {"dim": 8, "kappa": 3.2, "n_candidates": 50000},
}

BASE_DIR = "initial_data"  # folder containing function_1, ... function_8

# For high-dim NN optimisation
NN_CONFIG = {
    "hidden": (64, 64),
    "epochs": 250,
    "batch_size": 32,
    "lr": 1e-3,
    "n_restarts": 64,        # number of starting points for gradient ascent
    "steps": 120,            # gradient steps per restart
    "step_size": 0.05,       # step size in input space
    "noise_std": 0.01,       # add tiny noise during ascent to escape flats
}

# -----------------------------
# Portal format
# -----------------------------
def format_query(x):
    return "-".join(f"{float(v):.6f}" for v in x)

# -----------------------------
# GP-UCB acquisition
# -----------------------------
def ucb_acquisition(X_cand, gp, kappa=2.5):
    mu, std = gp.predict(X_cand, return_std=True)
    return mu + kappa * std

def sobol_candidates(n, dim, seed):
    m = int(np.ceil(np.log2(n)))
    sampler = qmc.Sobol(d=dim, scramble=True, seed=seed)
    X = sampler.random(n=2**m)
    return X[:n]

# -----------------------------
# Fit GP surrogate (d<=5)
# -----------------------------
def gp_suggest(X, y, dim, kappa, n_candidates, seed):
    length_scale0 = np.full(dim, 0.2)
    kernel = (
        ConstantKernel(1.0, (0.01, 10.0)) *
        Matern(length_scale=length_scale0, length_scale_bounds=(1e-4, 10.0), nu=2.5)
        + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1.0))
    )

    gp = GaussianProcessRegressor(
        kernel=kernel,
        normalize_y=True,
        n_restarts_optimizer=5,
        random_state=seed,
        alpha=1e-10
    )
    gp.fit(X, y)

    X_cand = sobol_candidates(n_candidates, dim, seed)
    acq = ucb_acquisition(X_cand, gp, kappa=kappa)
    x_next = X_cand[int(np.argmax(acq))]
    return np.clip(x_next, 0.0, 1.0)

# -----------------------------
# Fit NN surrogate and do gradient ascent on input (d>=6)
# -----------------------------
def build_mlp(input_dim, hidden=(64, 64), lr=1e-3, seed=0):
    tf.keras.utils.set_random_seed(seed)
    model = models.Sequential()
    model.add(layers.Input(shape=(input_dim,)))
    for h in hidden:
        model.add(layers.Dense(h, activation="relu"))
    model.add(layers.Dense(1, activation=None))  # regression output
    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss="mse")
    return model

def nn_suggest(X, y, dim, seed):
    # Normalise y for stability (NN tends to train better)
    y_mean, y_std = float(np.mean(y)), float(np.std(y) + 1e-8)
    y_norm = (y - y_mean) / y_std

    model = build_mlp(
        input_dim=dim,
        hidden=NN_CONFIG["hidden"],
        lr=NN_CONFIG["lr"],
        seed=seed
    )
    model.fit(
        X, y_norm,
        epochs=NN_CONFIG["epochs"],
        batch_size=NN_CONFIG["batch_size"],
        verbose=0
    )

    # Gradient ascent on model prediction w.r.t input x in [0,1]^d
    rng = np.random.default_rng(seed)
    starts = rng.uniform(0.0, 1.0, size=(NN_CONFIG["n_restarts"], dim)).astype(np.float32)

    best_x = None
    best_val = -np.inf

    for s in starts:
        x_var = tf.Variable(s.reshape(1, -1), dtype=tf.float32)

        for _ in range(NN_CONFIG["steps"]):
            with tf.GradientTape() as tape:
                pred = model(x_var, training=False)  # shape (1,1)
                # We maximize pred; add tiny noise to avoid plateaus
                obj = pred + NN_CONFIG["noise_std"] * tf.random.normal(shape=(1,1), dtype=tf.float32)

            grad = tape.gradient(obj, x_var)  # shape (1,dim)
            x_var.assign_add(NN_CONFIG["step_size"] * tf.math.l2_normalize(grad, axis=1))
            x_var.assign(tf.clip_by_value(x_var, 0.0, 1.0))

        val = float(model(x_var, training=False).numpy().ravel()[0])
        if val > best_val:
            best_val = val
            best_x = x_var.numpy().ravel()

    return np.clip(best_x, 0.0, 1.0)

# -----------------------------
# One function suggestion
# -----------------------------
def suggest_next_point_for_function(func_id, seed=0):
    cfg = FUNCTION_CONFIG[func_id]
    dim = cfg["dim"]

    folder = f"{BASE_DIR}/function_{func_id}"
    X = np.load(f"{folder}/initial_inputs.npy")
    y = np.load(f"{folder}/initial_outputs.npy").ravel()

    if X.shape[1] != dim:
        raise ValueError(f"Function {func_id}: expected dim={dim}, got X shape {X.shape}")

    # Choose surrogate based on dimensionality
    if dim <= 5:
        x_next = gp_suggest(
            X, y,
            dim=dim,
            kappa=cfg["kappa"],
            n_candidates=cfg["n_candidates"],
            seed=seed
        )
    else:
        x_next = nn_suggest(X, y, dim=dim, seed=seed)

    return x_next, format_query(x_next)

# -----------------------------
# Run all functions
# -----------------------------
def suggest_for_all_functions():
    results = {}
    for func_id in range(1, 9):
        x_next, submission = suggest_next_point_for_function(func_id, seed=func_id)
        results[func_id] = {"x_next": x_next, "submission": submission}
        print(f"Function {func_id}: {submission}")
    return results

if __name__ == "__main__":
    suggest_for_all_functions()
