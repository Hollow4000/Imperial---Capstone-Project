import os
import numpy as np
from scipy.stats import qmc
from sklearn.kernel_ridge import KernelRidge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

# -----------------------------
# Config (baseline values)
# -----------------------------
FUNCTION_CONFIG = {
    1: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    2: {"dim": 2, "kappa": 2.2, "n_candidates": 20000},
    3: {"dim": 3, "kappa": 2.4, "n_candidates": 25000},
    4: {"dim": 4, "kappa": 2.6, "n_candidates": 30000},
    5: {"dim": 4, "kappa": 1.4, "n_candidates": 30000},
    6: {"dim": 5, "kappa": 2.6, "n_candidates": 30000},
    7: {"dim": 6, "kappa": 3.0, "n_candidates": 40000},
    8: {"dim": 8, "kappa": 3.2, "n_candidates": 50000},
}

BASE_DIR = "initial_data"

# Portal formatting constraints
CLIP_EPS = 1e-6  # keep strictly < 1.0 so each value starts with "0."

# Hyperparameter tuning grids (small but meaningful)
ALPHA_GRID = np.array([1e-4, 1e-3, 1e-2, 1e-1], dtype=float)
GAMMA_GRID = np.array([0.25, 0.5, 1.0, 2.0, 4.0], dtype=float)

# Base BO knobs (adapted with noise)
BASE_ENSEMBLE_B = 12
BASE_LOCAL_FRAC = 0.35
BASE_LOCAL_NOISE = 0.07

# CV settings
CV_SPLITS = 5

# -----------------------------
# NEW: "LLM-style decoding knobs"
# -----------------------------
DECODER = {
    "temperature_base": 0.65,   # lower => greedier; higher => more diverse
    "top_p": 0.15,              # nucleus: keep smallest set with cum prob >= top_p
    "top_k": 250,               # always cap to top_k points after sorting
    "attn_top_m": 3,            # attend to top-m best observed points (not just best-1)
    "attn_softmax_temp": 0.25,  # lower => heavier weight on best points
}

# -----------------------------
# Portal formatting
# -----------------------------
def format_query(x):
    x = np.clip(x, 0.0, 1.0 - CLIP_EPS)
    return "-".join(f"{float(v):.6f}" for v in x)

# -----------------------------
# Candidate generation
# -----------------------------
def sobol_candidates(n, dim, seed):
    m = int(np.ceil(np.log2(n)))
    sampler = qmc.Sobol(d=dim, scramble=True, seed=seed)
    X = sampler.random(n=2**m)
    return X[:n]

def local_candidates(x_center, n, seed, noise):
    rng = np.random.default_rng(seed)
    X = x_center[None, :] + rng.normal(0.0, noise, size=(n, x_center.size))
    return np.clip(X, 0.0, 1.0 - CLIP_EPS)

def softmax(z, temp=1.0):
    z = (z - np.max(z)) / max(temp, 1e-12)
    e = np.exp(z)
    return e / (np.sum(e) + 1e-12)

# -----------------------------
# Noise proxy + tuning
# -----------------------------
def estimate_noise_level(Xs, y, alpha, gamma, seed):
    n = Xs.shape[0]
    if n < 6:
        return 0.0
    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)
    errs = []
    for tr, va in kf.split(Xs):
        model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
        model.fit(Xs[tr], y[tr])
        pred = model.predict(Xs[va])
        errs.append(np.mean((pred - y[va]) ** 2))
    return float(np.mean(errs))

def tune_krr_hyperparams(Xs, y, seed):
    best = {"alpha": None, "gamma": None, "cv_mse": np.inf}
    n = Xs.shape[0]
    kf = KFold(n_splits=min(CV_SPLITS, n), shuffle=True, random_state=seed)

    for alpha in ALPHA_GRID:
        for gamma in GAMMA_GRID:
            mses = []
            for tr, va in kf.split(Xs):
                model = KernelRidge(kernel="rbf", alpha=alpha, gamma=gamma)
                model.fit(Xs[tr], y[tr])
                pred = model.predict(Xs[va])
                mses.append(np.mean((pred - y[va]) ** 2))
            cv_mse = float(np.mean(mses))
            if cv_mse < best["cv_mse"]:
                best = {"alpha": float(alpha), "gamma": float(gamma), "cv_mse": cv_mse}
    return best

# -----------------------------
# NEW: LLM-style decoding over candidates
# -----------------------------
def decode_choice(acq, rng, temperature=1.0, top_p=0.2, top_k=200):
    """
    Turn acquisition scores into a sampled index (not always argmax).
    - temperature: flattens/sharpens distribution over candidates
    - top_p: nucleus sampling on probabilities
    - top_k: hard cap on how many candidates remain
    """
    # Convert scores -> probabilities with temperature
    # Higher temperature => more uniform
    probs = softmax(acq, temp=max(temperature, 1e-6))

    # Sort descending by prob
    order = np.argsort(-probs)
    probs_sorted = probs[order]

    # Apply top-k
    k = min(top_k, len(probs_sorted))
    order = order[:k]
    probs_sorted = probs_sorted[:k]

    # Apply nucleus (top-p): keep minimal prefix reaching cumulative mass >= top_p
    cdf = np.cumsum(probs_sorted)
    cutoff = np.searchsorted(cdf, top_p)
    cutoff = int(np.clip(cutoff, 0, len(probs_sorted)-1))
    keep = cutoff + 1

    order = order[:keep]
    probs_keep = probs_sorted[:keep]
    probs_keep = probs_keep / (np.sum(probs_keep) + 1e-12)

    # Sample index
    return int(rng.choice(order, p=probs_keep))

# -----------------------------
# Kernel Ridge ensemble UCB + decoding
# -----------------------------
def kernel_ucb_suggest(X, y, dim, kappa_base, n_candidates, seed):
    rng = np.random.default_rng(seed)

    scaler = StandardScaler()
    Xs = scaler.fit_transform(X)

    tuned = tune_krr_hyperparams(Xs, y, seed=seed)
    alpha_star, gamma_star = tuned["alpha"], tuned["gamma"]

    noise_proxy = estimate_noise_level(Xs, y, alpha_star, gamma_star, seed=seed)
    noise_norm = noise_proxy / (noise_proxy + 1.0)

    # "emergence": ensemble gets bigger when noisy
    ENSEMBLE_B = int(np.clip(BASE_ENSEMBLE_B + 10 * noise_norm, 10, 24))

    # "attention": local search shrinks when noisy (more explore)
    LOCAL_FRAC = float(np.clip(BASE_LOCAL_FRAC - 0.20 * noise_norm, 0.15, 0.45))
    LOCAL_NOISE = float(np.clip(BASE_LOCAL_NOISE + 0.06 * noise_norm, 0.05, 0.14))

    # UCB exploration strength
    kappa = float(np.clip(kappa_base + 0.8 * noise_norm, 0.8, 4.0))

    # NEW: decode settings adapt like LLM generation
    # noisier => slightly higher temperature + larger top_p (more diverse)
    temperature = float(np.clip(DECODER["temperature_base"] + 0.5 * noise_norm, 0.4, 1.2))
    top_p = float(np.clip(DECODER["top_p"] + 0.10 * noise_norm, 0.08, 0.35))
    top_k = int(np.clip(DECODER["top_k"] + 400 * noise_norm, 150, 800))

    # "attention": use top-m good points as local anchors, not just best-1
    m = min(DECODER["attn_top_m"], len(y))
    top_idx = np.argsort(-y)[:m]
    weights = softmax(y[top_idx], temp=DECODER["attn_softmax_temp"])
    # allocate local candidates across anchors by weights
    n_local = int(n_candidates * LOCAL_FRAC)
    n_global = n_candidates - n_local

    X_global = sobol_candidates(n_global, dim, seed)
    locals_list = []
    # ensure at least 1 candidate per anchor if n_local > 0
    alloc = np.maximum(1, (weights * n_local).astype(int)) if n_local > 0 else np.array([], dtype=int)
    # fix rounding so total = n_local
    if n_local > 0:
        while alloc.sum() > n_local:
            alloc[np.argmax(alloc)] -= 1
        while alloc.sum() < n_local:
            alloc[np.argmax(weights)] += 1

        for j, idx in enumerate(top_idx):
            locals_list.append(local_candidates(X[idx], int(alloc[j]), seed + 1000 + j, noise=LOCAL_NOISE))

    X_local = np.vstack(locals_list) if locals_list else np.empty((0, dim))
    X_cand = np.vstack([X_global, X_local])
    Xc = scaler.transform(X_cand)

    # Bootstrap ensemble predictions
    preds = np.zeros((ENSEMBLE_B, Xc.shape[0]), dtype=float)
    n = Xs.shape[0]
    for b in range(ENSEMBLE_B):
        idx = rng.integers(0, n, size=n)
        model = KernelRidge(kernel="rbf", alpha=alpha_star, gamma=gamma_star)
        model.fit(Xs[idx], y[idx])
        preds[b] = model.predict(Xc)

    mu = preds.mean(axis=0)
    sigma = preds.std(axis=0)

    # distance-based diversity
    dists = np.min(np.linalg.norm(Xc[:, None, :] - Xs[None, :, :], axis=2), axis=1)
    dists = dists / (dists.max() + 1e-12)
    sigma = 0.85 * sigma + 0.15 * dists

    acq = mu + kappa * sigma

    # NEW: instead of argmax, use decoding to sample from high-acquisition region
    chosen_idx = decode_choice(acq, rng, temperature=temperature, top_p=top_p, top_k=top_k)
    x_next = np.clip(X_cand[chosen_idx], 0.0, 1.0 - CLIP_EPS)

    debug = {
        "alpha": alpha_star,
        "gamma": gamma_star,
        "cv_mse": tuned["cv_mse"],
        "noise_proxy": noise_proxy,
        "noise_norm": noise_norm,
        "ENSEMBLE_B": ENSEMBLE_B,
        "LOCAL_FRAC": LOCAL_FRAC,
        "LOCAL_NOISE": LOCAL_NOISE,
        "kappa_used": kappa,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "attn_top_m": int(m),
    }
    return x_next, debug

# -----------------------------
# One function runner
# -----------------------------
def suggest_next_point_for_function(func_id, seed):
    cfg = FUNCTION_CONFIG[func_id]
    dim = cfg["dim"]

    folder = os.path.join(BASE_DIR, f"function_{func_id}")
    X = np.load(os.path.join(folder, "initial_inputs.npy"))
    y = np.load(os.path.join(folder, "initial_outputs.npy")).ravel()

    if X.shape[1] != dim:
        raise ValueError(f"Dim mismatch for function {func_id}: expected {dim}, got {X.shape[1]}")

    x_next, debug = kernel_ucb_suggest(
        X, y,
        dim=dim,
        kappa_base=cfg["kappa"],
        n_candidates=cfg["n_candidates"],
        seed=seed
    )
    return x_next, format_query(x_next), debug

# -----------------------------
# Run all functions
# -----------------------------
def suggest_for_all_functions():
    results = {}
    debug_all = {}

    for func_id in range(1, 9):
        x_next, submission, debug = suggest_next_point_for_function(func_id, seed=func_id)
        results[func_id] = submission
        debug_all[func_id] = debug

        print(f"Function {func_id}: {submission}")
        print(f"  tuned alpha={debug['alpha']}, gamma={debug['gamma']}, cv_mse={debug['cv_mse']:.6g}")
        print(f"  noise_proxy={debug['noise_proxy']:.6g} -> kappa_used={debug['kappa_used']:.3f}")
        print(f"  ensemble={debug['ENSEMBLE_B']}, local_frac={debug['LOCAL_FRAC']:.2f}, local_noise={debug['LOCAL_NOISE']:.3f}")
        print(f"  decoder: temp={debug['temperature']:.3f}, top_p={debug['top_p']:.3f}, top_k={debug['top_k']}, attn_top_m={debug['attn_top_m']}")
        print()

    return results, debug_all

if __name__ == "__main__":
    suggest_for_all_functions()
